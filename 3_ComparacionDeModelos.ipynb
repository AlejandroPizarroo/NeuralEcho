{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Comparación de Modelos\n",
        "\n",
        "<h3> Creación de gráficos y GIFs para comparar entre ambos tipos de modelos.\n",
        "\n",
        "Equipo de Reto 5\n",
        "\n",
        "Inteligencia artificial avanzada para la ciencia de datos II (Grupo 502)\n",
        "\n",
        "30 de noviembre de 2023"
      ],
      "metadata": {
        "id": "BmP6bC4TNlfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparar el entorno de trabajo"
      ],
      "metadata": {
        "id": "2mR0SDQvN9oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerias y módulos necesarios\n",
        "from keras.models import load_model\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import imageio"
      ],
      "metadata": {
        "id": "e4_81d8D5Rhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el directorio de trabajo actual\n",
        "os.chdir('/content/drive/Shareddrives/Penta Tech/Reto')"
      ],
      "metadata": {
        "id": "5WfDsFkEOUvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el directorio con los mejores modelos\n",
        "models_directory = os.path.join(os.getcwd(), 'Models')"
      ],
      "metadata": {
        "id": "7WMol8WqOVE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el directorio para guardar los GIFs generados\n",
        "gifs_directory = os.path.join(os.getcwd(), 'GIFs')\n",
        "os.mkdir(gifs_directory)"
      ],
      "metadata": {
        "id": "8xoO4dmXKNm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir dimensiones importantes de los datos\n",
        "img_height = 112\n",
        "img_width = 112\n",
        "img_channels = 1\n",
        "mask_channels = 1\n",
        "landmarks_channels = 7"
      ],
      "metadata": {
        "id": "EEaVq06FylvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los mejores modelos\n",
        "mask_model = load_model(os.path.join(models_directory, 'masks_08_0.99.h5'), compile=False)\n",
        "landmarks_model = load_model(os.path.join(models_directory, 'landmarks_05_0.45.h5'), compile=False)"
      ],
      "metadata": {
        "id": "icjPfiW23QBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcular el coeficiente de Sorensen-Dice para todos los datos del conjunto de prueba\n",
        "\n"
      ],
      "metadata": {
        "id": "IqtlWzbkDchC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el directorio con los conjuntos de datos\n",
        "data_directory = os.path.join(os.getcwd(), 'CompleteDatasets')"
      ],
      "metadata": {
        "id": "my6a3xA7QUwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de prueba\n",
        "test_images = np.load('/content/drive/Shareddrives/Penta Tech/Reto/CompleteDatasets/test_images.npy')\n",
        "test_masks = np.load('/content/drive/Shareddrives/Penta Tech/Reto/CompleteDatasets/test_masks.npy')\n",
        "print('Dimensiones del conjunto de prueba:', test_images.shape, test_masks.shape)"
      ],
      "metadata": {
        "id": "VQshPbvnQK4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Predecir con ambos modelos todos los datos en el conjunto de prueba\n",
        "mask_predictions = mask_model.predict(test_images)\n",
        "landmark_predictions = landmarks_model.predict(test_images)"
      ],
      "metadata": {
        "id": "OhmWBOuJQMCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear listas para guardar los coeficientes\n",
        "dice_scores_masks = []\n",
        "dice_scores_landmarks = []\n",
        "\n",
        "# Para cada predicción del conjunto de prueba\n",
        "for prediction in range(len(mask_predictions)):\n",
        "\n",
        "  # Seleccionar las predicciones correspondiente\n",
        "  true_prediction = test_masks[prediction]\n",
        "  mask_prediction = (mask_predictions[prediction] > 0.5)\n",
        "  landmark_prediction = landmark_predictions[prediction]\n",
        "\n",
        "  # Generar la máscara de la predicción basada en puntos de interés\n",
        "  coords = []\n",
        "  for channel in range(landmark_prediction.shape[2]):\n",
        "    landmark = (landmark_prediction[:,:,channel] >= np.max(landmark_prediction[:,:,channel]))\n",
        "    landmark = np.unravel_index(np.argmax(landmark), landmark.shape)\n",
        "    landmark = (landmark[1], landmark[0])\n",
        "    if landmark != (0, 0):\n",
        "      coords.append(landmark)\n",
        "  mask = np.zeros_like(mask_prediction).astype(np.uint8)\n",
        "  coords = np.array(coords).astype(np.int32).reshape((-1, 1, 2))\n",
        "  cv2.fillPoly(mask, [cv2.convexHull(coords)], (255))\n",
        "  landmark_prediction = (mask / 255.0).astype(np.float16)\n",
        "\n",
        "  # Añadir los coeficientes a su lista correspondiente\n",
        "  mask_intersection = np.sum((true_prediction == 1) & (mask_prediction == 1))\n",
        "  mask_total = np.sum(true_prediction == 1) + np.sum(mask_prediction == 1)\n",
        "  landmark_intersection = np.sum((true_prediction == 1) & (landmark_prediction == 1))\n",
        "  landmark_total = np.sum(true_prediction == 1) + np.sum(landmark_prediction == 1)\n",
        "  dice_scores_masks.append(float(((2*mask_intersection)/mask_total)*100))\n",
        "  dice_scores_landmarks.append(float(((2*landmark_intersection)/landmark_total)*100))"
      ],
      "metadata": {
        "id": "OKEauJ3AQfSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un histograma de con todos los coeficientes del conjunto de prueba\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.hist(dice_scores_masks, bins=np.arange(min(dice_scores_masks), max(dice_scores_masks) + 0.5, 0.5), alpha=0.6, label=f'Mask Model (Avg: {statistics.mean(dice_scores_masks):.3f})')\n",
        "plt.hist(dice_scores_landmarks, bins=np.arange(min(dice_scores_landmarks), max(dice_scores_landmarks) + 0.5, 0.5), alpha=0.6, label=f'Landmark Model (Avg: {statistics.mean(dice_scores_landmarks):.3f})')\n",
        "plt.title('Dice Score Distribution')\n",
        "plt.xlabel('Dice Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(alpha = 0.5)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "DqOhFR90Bhw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear GIFs con las máscaras generadas por cada modelo"
      ],
      "metadata": {
        "id": "i2FmqkLbQfvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista con videos que pertenecen al conjunto de datos externos\n",
        "apical_files = os.listdir(os.path.join(os.getcwd(), 'Apical'))"
      ],
      "metadata": {
        "id": "WyAhA3Wu_3xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista con videos que pertenecen al conjunto de datos de prueba\n",
        "test_files = pd.read_csv(os.path.join(os.getcwd(), 'EchoNet-Dynamic/FileList.csv'))\n",
        "test_files.drop(columns=['EF', 'ESV', 'EDV', 'FrameHeight', 'FrameWidth', 'FPS', 'NumberOfFrames'], inplace=True)\n",
        "test_files = list(test_files[test_files['Split'] == 'TEST'].sample(n=len(apical_files)).FileName.values)"
      ],
      "metadata": {
        "id": "KLRq6qoP98BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar diez elementos aleatorios de las listas con datos externos y de prueba\n",
        "test_files = random.sample(apical_files, 5) + random.sample(test_files, 5)\n",
        "\n",
        "# Para diez videos aleatorios en la lista\n",
        "for video in test_files:\n",
        "\n",
        "  # Definir la ubicación del video dependiendo del conjunto al que pertenece\n",
        "  if len(video) == 9:\n",
        "    path = os.path.join(os.getcwd(), 'Apical', video)\n",
        "  else:\n",
        "    video = video + '.avi'\n",
        "    path = os.path.join(os.getcwd(), 'EchoNet-Dynamic/Videos', video)\n",
        "\n",
        "  # Guardar todos los fotogramas del video en un arreglo\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  frames = np.empty((0, img_height, img_width, img_channels), dtype=np.float16)\n",
        "  for f in range(frame_count):\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
        "    _, frame = cap.read()\n",
        "    frame = (cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
        "    if frame.shape != (img_height, img_width):\n",
        "      frame = cv2.resize(frame, (img_height, img_height))\n",
        "    frame = (frame / 255.0).astype(np.float16)\n",
        "    frames = np.append(frames, [np.expand_dims(frame, axis=-1)], axis=0)\n",
        "  cap.release()\n",
        "\n",
        "  # Imprimir la información del video\n",
        "  print(f'VIDEO: {video[:-4]}.gif  FOTOGRAMAS: {frame_count}  FPS: {fps}')\n",
        "\n",
        "  # Generar predicciones para todos los fotogramas del video con ambos modelo\n",
        "  mask_predictions = mask_model.predict(frames)\n",
        "  landmark_predictions = landmarks_model.predict(frames)\n",
        "\n",
        "  # Guardar en una lista todos los fotogramas con la máscaras\n",
        "  mask_frames = []\n",
        "  for frame_number in range(frame_count):\n",
        "    frame = (frames[frame_number] * 255).astype(np.uint8)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "    mask = np.squeeze((mask_predictions[frame_number] > 0.5) * 255).astype(np.uint8)\n",
        "    frame[:,:,2] = cv2.addWeighted(mask, 0.75, frame[:,:,2], 1, 0)\n",
        "    cv2.putText(frame, ('M1 ' + video[:-4]), (1,img_height-3), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (255, 255, 255), 1)\n",
        "    mask_frames.append(frame)\n",
        "\n",
        "  # Guardar en una lista todos los fotogramas con los puntos de interés\n",
        "  landmark_frames = []\n",
        "  for frame_number in range(frame_count):\n",
        "    coords = []\n",
        "    frame = (frames[frame_number] * 255).astype(np.uint8)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "    landmarks = landmark_predictions[frame_number]\n",
        "    for channel in range(landmarks.shape[2]):\n",
        "      landmark = (landmarks[:,:,channel] >= np.max(landmarks[:,:,channel]))\n",
        "      landmark = np.unravel_index(np.argmax(landmark), landmark.shape)\n",
        "      landmark = (landmark[1], landmark[0])\n",
        "      coords.append(landmark)\n",
        "      cv2.circle(frame, landmark, 2, 255, -1)\n",
        "    coords = np.array(coords).astype(np.int32).reshape((-1, 1, 2))\n",
        "    mask = np.zeros_like(mask).astype(np.int32)\n",
        "    cv2.fillPoly(mask, [cv2.convexHull(coords)], (255))\n",
        "    frame[:,:,2] = cv2.addWeighted(mask.astype(np.uint8), 0.75, frame[:,:,2], 1, 0)\n",
        "    cv2.putText(frame, ('L1 ' + video[:-4]), (1,img_height-3), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (255, 255, 255), 1)\n",
        "    landmark_frames.append(frame)\n",
        "\n",
        "  # Guardar los fotogramas con las predicciones como GIFs\n",
        "  imageio.mimsave(os.path.join(gifs_directory, (video[:-4] + '_mask.gif')), mask_frames, fps=fps)\n",
        "  imageio.mimsave(os.path.join(gifs_directory, (video[:-4] + '_landmarks.gif')), landmark_frames, fps=fps)"
      ],
      "metadata": {
        "id": "b1KPPxgrAP7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}